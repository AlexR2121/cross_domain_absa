{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform from xml to python format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "\n",
    "    Doc\n",
    ")\n",
    "\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Функции для получения разметки из xml файлов SentiRuEval\n",
    "\n",
    "def parse_texts(path):\n",
    "    root_node = ET.parse(path).getroot()\n",
    "    texts = dict()\n",
    "    for rev in root_node.findall('review'):\n",
    "        texts[rev.get('id')] = rev.find('text').text\n",
    "    return texts\n",
    "\n",
    "def parse_aspects(path):\n",
    "    root_node = ET.parse(path).getroot()\n",
    "    global_aspects = dict()\n",
    "    tag_list = ['type', 'to','term','sentiment','mark','from','category']\n",
    "    for rev in root_node.findall('review'):\n",
    "        id_ = rev.get('id')\n",
    "        aspects = dict()\n",
    "        for i, a in enumerate(rev.findall('aspects/aspect')):\n",
    "            aspects[i] = dict()\n",
    "            for tag in tag_list:\n",
    "                aspects[i][f'{tag}'] = a.get(tag)\n",
    "        global_aspects[f'{id_}'] = aspects\n",
    "    return global_aspects\n",
    "\n",
    "def mark_text(path, aspect_type = 'explicit', binary = True):\n",
    "    global_aspects = parse_aspects(path)\n",
    "    texts = parse_texts(path)\n",
    "    \n",
    "    categories = set()\n",
    "    for id_ in global_aspects.values():\n",
    "        for aspect in id_.values():\n",
    "            categories.add(aspect['category'])\n",
    "    \n",
    "    markup = dict()\n",
    "    for id_, aspects in global_aspects.items():\n",
    "        labels = []\n",
    "        tokens = list(tokenize(texts[id_]))\n",
    "        for token in tokens:\n",
    "            label = 0\n",
    "            for aspect in aspects.values():\n",
    "                if aspect['type'] == aspect_type and aspect['mark'] == 'Rel':\n",
    "                    if token.start == int(aspect['from']):\n",
    "                        label = 1\n",
    "                    elif token.start > int(aspect['from']) and token.stop <= int(aspect['to']):\n",
    "                        if binary:\n",
    "                            label = 1\n",
    "                        else:\n",
    "                            label = 2\n",
    "            labels.append(label)\n",
    "        markup[id_] = labels\n",
    "        \n",
    "    return markup, texts\n",
    "\n",
    "def markup_text_senti(path):\n",
    "    global_aspects = parse_aspects(path)\n",
    "    texts = parse_texts(path)\n",
    "    markup = defaultdict(dict)\n",
    "    for id_, aspects in global_aspects.items():\n",
    "        spans = []\n",
    "        markup[id_]['text'] = texts[id_]\n",
    "        for entity in aspects.values():\n",
    "            spans.append((int(entity['from']), int(entity['to']), entity['polarity']))\n",
    "            spans.append((int(entity['from']), int(entity['to']), entity['category']))\n",
    "        markup[id_]['aspects'] = spans\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "senti_texts_train = parse_texts('data/SentiRuEval_car_markup_train.xml')\n",
    "senti_texts_test = parse_texts('data/SentiRuEval_car_markup_test.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_texts = {**senti_texts_train,**senti_texts_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_aspects_train = parse_aspects('data/SentiRuEval_car_markup_train.xml')\n",
    "senti_aspects_test = parse_aspects('data/SentiRuEval_car_markup_test.xml')\n",
    "senti_aspects = {**senti_aspects_train,**senti_aspects_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spans_senti_train = dict()\n",
    "for id_ in senti_aspects_train.keys():\n",
    "    entities = []\n",
    "    for entity in senti_aspects_train[id_].values():\n",
    "        if entity['type'] == 'explicit':\n",
    "            entities.append((int(entity['from']), int(entity['to']), entity['term'], entity['sentiment'], entity['category']))\n",
    "    spans_senti_train[id_] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spans_senti_test = dict()\n",
    "for id_ in senti_aspects_test.keys():\n",
    "    entities = []\n",
    "    for entity in senti_aspects_test[id_].values():\n",
    "        if entity['type'] == 'explicit':\n",
    "            entities.append((int(entity['from']), int(entity['to']), entity['term'], entity['sentiment'], entity['category']))\n",
    "    spans_senti_test[id_] = entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "senti_train_data = {}\n",
    "senti_test_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, text in senti_texts_train.items():\n",
    "    senti_train_data[k] = {}\n",
    "    senti_train_data[k]['text'] = text\n",
    "    senti_train_data[k]['spans'] = spans_senti_train[k]\n",
    "for k, text in senti_texts_test.items():\n",
    "    senti_test_data[k] = {}\n",
    "    senti_test_data[k]['text'] = text\n",
    "    senti_test_data[k]['spans'] = spans_senti_test[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Недавно купил этот автомобиль. Авто отличное! Двигатель 2,5 литра, турбодизель. Прежний хозяин сказал при продаже, что масло не жрет, солярку тоже, летит как угорелая! Так оно и есть. 140 км/ч нормальная крейсерская скорость. Вообще немцы умеют делать автомобили. Дорогу держит отлично, так как достаточно широкая машина. Тормоза все дисковые. Главное передний привод, по сравнению с другими немецкими автомобилями. Такими как мерседес и бмв этого же класса. Места в автомобиле очень много. Не тесно даже, когда сидят пять взрослых человек. Багажное отделение тоже огромно. Влезла стиральная машина. По соотношению цена - качество, отличный автомобиль. Больше никогда не сяду за руль российского автомбиля! Всем рекомендую Ауди. '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senti_texts_train['92845']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Doc(senti_texts_train['92845'])\n",
    "doc.segment(segmenter)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "doc.tag_morph(morph_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DocToken(stop=7, text='Недавно', id='1_1', head_id='1_2', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=8, stop=13, text='купил', id='1_2', head_id='1_0', rel='root', pos='VERB', feats=<Perf,Masc,Ind,Sing,Past,Fin,Act>),\n",
       " DocToken(start=14, stop=18, text='этот', id='1_3', head_id='1_4', rel='det', pos='DET', feats=<Acc,Masc,Sing>),\n",
       " DocToken(start=19, stop=29, text='автомобиль', id='1_4', head_id='1_2', rel='obj', pos='NOUN', feats=<Inan,Acc,Masc,Sing>),\n",
       " DocToken(start=29, stop=30, text='.', id='1_5', head_id='1_2', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=31, stop=35, text='Авто', id='2_1', head_id='2_2', rel='nsubj', pos='PROPN', feats=<Inan,Gen,Neut,Sing>),\n",
       " DocToken(start=36, stop=44, text='отличное', id='2_2', head_id='2_0', rel='root', pos='ADJ', feats=<Nom,Pos,Neut,Sing>),\n",
       " DocToken(start=44, stop=45, text='!', id='2_3', head_id='2_2', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=46, stop=55, text='Двигатель', id='3_1', head_id='3_5', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=56, stop=59, text='2,5', id='3_2', head_id='3_3', rel='nummod', pos='NUM'),\n",
       " DocToken(start=60, stop=65, text='литра', id='3_3', head_id='3_1', rel='nmod', pos='NOUN', feats=<Inan,Gen,Masc,Sing>),\n",
       " DocToken(start=65, stop=66, text=',', id='3_4', head_id='3_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=67, stop=78, text='турбодизель', id='3_5', head_id='3_1', rel='conj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=78, stop=79, text='.', id='3_6', head_id='3_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=80, stop=87, text='Прежний', id='4_1', head_id='4_2', rel='amod', pos='ADJ', feats=<Nom,Pos,Masc,Sing>),\n",
       " DocToken(start=88, stop=94, text='хозяин', id='4_2', head_id='4_3', rel='nsubj', pos='NOUN', feats=<Anim,Nom,Masc,Sing>),\n",
       " DocToken(start=95, stop=101, text='сказал', id='4_3', head_id='4_0', rel='root', pos='VERB', feats=<Perf,Masc,Ind,Sing,Past,Fin,Act>),\n",
       " DocToken(start=102, stop=105, text='при', id='4_4', head_id='4_5', rel='case', pos='ADP'),\n",
       " DocToken(start=106, stop=113, text='продаже', id='4_5', head_id='4_3', rel='obl', pos='NOUN', feats=<Inan,Loc,Fem,Sing>),\n",
       " DocToken(start=113, stop=114, text=',', id='4_6', head_id='4_10', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=115, stop=118, text='что', id='4_7', head_id='4_10', rel='mark', pos='SCONJ'),\n",
       " DocToken(start=119, stop=124, text='масло', id='4_8', head_id='4_10', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Neut,Sing>),\n",
       " DocToken(start=125, stop=127, text='не', id='4_9', head_id='4_10', rel='advmod', pos='PART', feats=<Neg>),\n",
       " DocToken(start=128, stop=132, text='жрет', id='4_10', head_id='4_3', rel='ccomp', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>),\n",
       " DocToken(start=132, stop=133, text=',', id='4_11', head_id='4_12', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=134, stop=141, text='солярку', id='4_12', head_id='4_10', rel='conj', pos='NOUN', feats=<Inan,Acc,Fem,Sing>),\n",
       " DocToken(start=142, stop=146, text='тоже', id='4_13', head_id='4_12', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=146, stop=147, text=',', id='4_14', head_id='4_12', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=148, stop=153, text='летит', id='4_15', head_id='4_3', rel='conj', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>),\n",
       " DocToken(start=154, stop=157, text='как', id='4_16', head_id='4_17', rel='case', pos='SCONJ'),\n",
       " DocToken(start=158, stop=166, text='угорелая', id='4_17', head_id='4_15', rel='obl', pos='VERB', feats=<Imp,Imp,Sing,2,Fin,Act>),\n",
       " DocToken(start=166, stop=167, text='!', id='4_18', head_id='4_3', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=168, stop=171, text='Так', id='5_1', head_id='5_4', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=172, stop=175, text='оно', id='5_2', head_id='5_4', rel='nsubj', pos='PRON', feats=<Nom,Neut,Sing,3>),\n",
       " DocToken(start=176, stop=177, text='и', id='5_3', head_id='5_4', rel='advmod', pos='PART'),\n",
       " DocToken(start=178, stop=182, text='есть', id='5_4', head_id='5_0', rel='root', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>),\n",
       " DocToken(start=182, stop=183, text='.', id='5_5', head_id='5_4', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=184, stop=187, text='140', id='6_1', head_id='6_2', rel='nummod', pos='NUM'),\n",
       " DocToken(start=188, stop=190, text='км', id='6_2', head_id='6_0', rel='root', pos='NOUN', feats=<Inan,Gen,Masc,Plur>),\n",
       " DocToken(start=190, stop=191, text='/', id='6_3', head_id='6_4', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=191, stop=192, text='ч', id='6_4', head_id='6_2', rel='nummod', pos='NOUN', feats=<Inan,Nom,Fem,Sing>),\n",
       " DocToken(start=193, stop=203, text='нормальная', id='6_5', head_id='6_7', rel='amod', pos='ADJ', feats=<Nom,Pos,Fem,Sing>),\n",
       " DocToken(start=204, stop=215, text='крейсерская', id='6_6', head_id='6_7', rel='amod', pos='ADJ', feats=<Nom,Pos,Fem,Sing>),\n",
       " DocToken(start=216, stop=224, text='скорость', id='6_7', head_id='6_2', rel='conj', pos='NOUN', feats=<Inan,Nom,Fem,Sing>),\n",
       " DocToken(start=224, stop=225, text='.', id='6_8', head_id='6_2', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=226, stop=232, text='Вообще', id='7_1', head_id='7_3', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=233, stop=238, text='немцы', id='7_2', head_id='7_3', rel='nsubj', pos='NOUN', feats=<Anim,Nom,Masc,Plur>),\n",
       " DocToken(start=239, stop=244, text='умеют', id='7_3', head_id='7_0', rel='root', pos='VERB', feats=<Imp,Ind,Plur,3,Pres,Fin,Act>),\n",
       " DocToken(start=245, stop=251, text='делать', id='7_4', head_id='7_3', rel='xcomp', pos='VERB', feats=<Imp,Inf,Act>),\n",
       " DocToken(start=252, stop=262, text='автомобили', id='7_5', head_id='7_4', rel='obj', pos='NOUN', feats=<Inan,Acc,Masc,Plur>),\n",
       " DocToken(start=262, stop=263, text='.', id='7_6', head_id='7_3', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=264, stop=270, text='Дорогу', id='8_1', head_id='8_2', rel='obj', pos='NOUN', feats=<Inan,Acc,Fem,Sing>),\n",
       " DocToken(start=271, stop=277, text='держит', id='8_2', head_id='8_0', rel='root', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>),\n",
       " DocToken(start=278, stop=285, text='отлично', id='8_3', head_id='8_2', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=285, stop=286, text=',', id='8_4', head_id='8_9', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=287, stop=290, text='так', id='8_5', head_id='8_9', rel='mark', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=291, stop=294, text='как', id='8_6', head_id='8_5', rel='fixed', pos='SCONJ'),\n",
       " DocToken(start=295, stop=305, text='достаточно', id='8_7', head_id='8_8', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=306, stop=313, text='широкая', id='8_8', head_id='8_9', rel='amod', pos='ADJ', feats=<Nom,Pos,Fem,Sing>),\n",
       " DocToken(start=314, stop=320, text='машина', id='8_9', head_id='8_7', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Fem,Sing>),\n",
       " DocToken(start=320, stop=321, text='.', id='8_10', head_id='8_2', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=322, stop=329, text='Тормоза', id='9_1', head_id='9_0', rel='root', pos='NOUN', feats=<Inan,Nom,Masc,Plur>),\n",
       " DocToken(start=330, stop=333, text='все', id='9_2', head_id='9_3', rel='det', pos='DET', feats=<Nom,Plur>),\n",
       " DocToken(start=334, stop=342, text='дисковые', id='9_3', head_id='9_1', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Masc,Plur>),\n",
       " DocToken(start=342, stop=343, text='.', id='9_4', head_id='9_1', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=344, stop=351, text='Главное', id='10_1', head_id='10_0', rel='root', pos='NOUN', feats=<Inan,Nom,Neut,Sing>),\n",
       " DocToken(start=352, stop=360, text='передний', id='10_2', head_id='10_3', rel='amod', pos='ADJ', feats=<Nom,Pos,Masc,Sing>),\n",
       " DocToken(start=361, stop=367, text='привод', id='10_3', head_id='10_1', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=367, stop=368, text=',', id='10_4', head_id='10_3', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=369, stop=371, text='по', id='10_5', head_id='10_10', rel='case', pos='ADP'),\n",
       " DocToken(start=372, stop=381, text='сравнению', id='10_6', head_id='10_5', rel='fixed', pos='NOUN', feats=<Inan,Dat,Neut,Sing>),\n",
       " DocToken(start=382, stop=383, text='с', id='10_7', head_id='10_5', rel='fixed', pos='ADP'),\n",
       " DocToken(start=384, stop=391, text='другими', id='10_8', head_id='10_10', rel='amod', pos='ADJ', feats=<Ins,Pos,Plur>),\n",
       " DocToken(start=392, stop=401, text='немецкими', id='10_9', head_id='10_10', rel='amod', pos='ADJ', feats=<Ins,Pos,Plur>),\n",
       " DocToken(start=402, stop=414, text='автомобилями', id='10_10', head_id='10_3', rel='nmod', pos='NOUN', feats=<Inan,Ins,Masc,Plur>),\n",
       " DocToken(start=414, stop=415, text='.', id='10_11', head_id='10_1', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=416, stop=422, text='Такими', id='11_1', head_id='11_0', rel='root', pos='DET', feats=<Ins,Plur>),\n",
       " DocToken(start=423, stop=426, text='как', id='11_2', head_id='11_3', rel='case', pos='SCONJ'),\n",
       " DocToken(start=427, stop=435, text='мерседес', id='11_3', head_id='11_1', rel='obl', pos='NOUN', feats=<Inan,Acc,Masc,Sing>),\n",
       " DocToken(start=436, stop=437, text='и', id='11_4', head_id='11_5', rel='cc', pos='CCONJ'),\n",
       " DocToken(start=438, stop=441, text='бмв', id='11_5', head_id='11_3', rel='conj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=442, stop=447, text='этого', id='11_6', head_id='11_8', rel='det', pos='DET', feats=<Gen,Masc,Sing>),\n",
       " DocToken(start=448, stop=450, text='же', id='11_7', head_id='11_6', rel='advmod', pos='PART'),\n",
       " DocToken(start=451, stop=457, text='класса', id='11_8', head_id='11_5', rel='nmod', pos='NOUN', feats=<Inan,Gen,Masc,Sing>),\n",
       " DocToken(start=457, stop=458, text='.', id='11_9', head_id='11_1', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=459, stop=464, text='Места', id='12_1', head_id='12_5', rel='nsubj', pos='NOUN', feats=<Inan,Gen,Neut,Sing>),\n",
       " DocToken(start=465, stop=466, text='в', id='12_2', head_id='12_3', rel='case', pos='ADP'),\n",
       " DocToken(start=467, stop=477, text='автомобиле', id='12_3', head_id='12_1', rel='nmod', pos='NOUN', feats=<Inan,Loc,Masc,Sing>),\n",
       " DocToken(start=478, stop=483, text='очень', id='12_4', head_id='12_5', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=484, stop=489, text='много', id='12_5', head_id='12_0', rel='root', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=489, stop=490, text='.', id='12_6', head_id='12_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=491, stop=493, text='Не', id='13_1', head_id='13_2', rel='advmod', pos='PART', feats=<Neg>),\n",
       " DocToken(start=494, stop=499, text='тесно', id='13_2', head_id='13_0', rel='root', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=500, stop=504, text='даже', id='13_3', head_id='13_2', rel='advmod', pos='PART'),\n",
       " DocToken(start=504, stop=505, text=',', id='13_4', head_id='13_6', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=506, stop=511, text='когда', id='13_5', head_id='13_6', rel='mark', pos='SCONJ'),\n",
       " DocToken(start=512, stop=517, text='сидят', id='13_6', head_id='13_3', rel='acl:relcl', pos='VERB', feats=<Imp,Ind,Plur,3,Pres,Fin,Act>),\n",
       " DocToken(start=518, stop=522, text='пять', id='13_7', head_id='13_9', rel='nummod:gov', pos='NUM', feats=<Nom>),\n",
       " DocToken(start=523, stop=531, text='взрослых', id='13_8', head_id='13_9', rel='amod', pos='ADJ', feats=<Gen,Pos,Plur>),\n",
       " DocToken(start=532, stop=539, text='человек', id='13_9', head_id='13_6', rel='nsubj', pos='NOUN', feats=<Anim,Gen,Masc,Plur>),\n",
       " DocToken(start=539, stop=540, text='.', id='13_10', head_id='13_2', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=541, stop=549, text='Багажное', id='14_1', head_id='14_2', rel='amod', pos='ADJ', feats=<Nom,Pos,Neut,Sing>),\n",
       " DocToken(start=550, stop=559, text='отделение', id='14_2', head_id='14_4', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Neut,Sing>),\n",
       " DocToken(start=560, stop=564, text='тоже', id='14_3', head_id='14_4', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=565, stop=572, text='огромно', id='14_4', head_id='14_0', rel='root', pos='ADJ', feats=<Pos,Neut,Sing,Short>),\n",
       " DocToken(start=572, stop=573, text='.', id='14_5', head_id='14_4', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=574, stop=580, text='Влезла', id='15_1', head_id='15_0', rel='root', pos='VERB', feats=<Imp,Fem,Ind,Sing,Past,Fin,Act>),\n",
       " DocToken(start=581, stop=591, text='стиральная', id='15_2', head_id='15_3', rel='amod', pos='ADJ', feats=<Nom,Pos,Fem,Sing>),\n",
       " DocToken(start=592, stop=598, text='машина', id='15_3', head_id='15_1', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Fem,Sing>),\n",
       " DocToken(start=598, stop=599, text='.', id='15_4', head_id='15_1', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=600, stop=602, text='По', id='16_1', head_id='16_2', rel='case', pos='ADP'),\n",
       " DocToken(start=603, stop=614, text='соотношению', id='16_2', head_id='16_5', rel='nmod', pos='NOUN', feats=<Inan,Dat,Neut,Sing>),\n",
       " DocToken(start=615, stop=619, text='цена', id='16_3', head_id='16_5', rel='nsubj', pos='NOUN', feats=<Inan,Nom,Fem,Sing>),\n",
       " DocToken(start=620, stop=621, text='-', id='16_4', head_id='16_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=622, stop=630, text='качество', id='16_5', head_id='16_3', rel='parataxis', pos='NOUN', feats=<Inan,Nom,Neut,Sing>),\n",
       " DocToken(start=630, stop=631, text=',', id='16_6', head_id='16_8', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=632, stop=640, text='отличный', id='16_7', head_id='16_8', rel='amod', pos='ADJ', feats=<Nom,Pos,Masc,Sing>),\n",
       " DocToken(start=641, stop=651, text='автомобиль', id='16_8', head_id='16_5', rel='conj', pos='NOUN', feats=<Inan,Nom,Masc,Sing>),\n",
       " DocToken(start=651, stop=652, text='.', id='16_9', head_id='16_5', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=653, stop=659, text='Больше', id='17_1', head_id='17_2', rel='advmod', pos='ADV', feats=<Cmp>),\n",
       " DocToken(start=660, stop=667, text='никогда', id='17_2', head_id='17_4', rel='advmod', pos='ADV', feats=<Pos>),\n",
       " DocToken(start=668, stop=670, text='не', id='17_3', head_id='17_4', rel='advmod', pos='PART', feats=<Neg>),\n",
       " DocToken(start=671, stop=675, text='сяду', id='17_4', head_id='17_0', rel='root', pos='VERB', feats=<Perf,Ind,Sing,1,Fut,Fin,Act>),\n",
       " DocToken(start=676, stop=678, text='за', id='17_5', head_id='17_6', rel='case', pos='ADP'),\n",
       " DocToken(start=679, stop=683, text='руль', id='17_6', head_id='17_4', rel='obl', pos='NOUN', feats=<Inan,Acc,Masc,Sing>),\n",
       " DocToken(start=684, stop=695, text='российского', id='17_7', head_id='17_8', rel='amod', pos='ADJ', feats=<Gen,Pos,Masc,Sing>),\n",
       " DocToken(start=696, stop=705, text='автомбиля', id='17_8', head_id='17_6', rel='nmod', pos='NOUN', feats=<Inan,Gen,Masc,Sing>),\n",
       " DocToken(start=705, stop=706, text='!', id='17_9', head_id='17_4', rel='punct', pos='PUNCT'),\n",
       " DocToken(start=707, stop=711, text='Всем', id='18_1', head_id='18_2', rel='iobj', pos='PRON', feats=<Anim,Dat,Plur>),\n",
       " DocToken(start=712, stop=722, text='рекомендую', id='18_2', head_id='18_0', rel='root', pos='VERB', feats=<Imp,Ind,Sing,1,Pres,Fin,Act>),\n",
       " DocToken(start=723, stop=727, text='Ауди', id='18_3', head_id='18_2', rel='nsubj', pos='PROPN', feats=<Anim,Dat,Masc,Sing>),\n",
       " DocToken(start=727, stop=728, text='.', id='18_4', head_id='18_2', rel='punct', pos='PUNCT')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS and Dep parcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_table = {'O': 0,\n",
    "             'B-positive': 1,\n",
    "             'B-neutral': 3,\n",
    "             'B-negative': 5,\n",
    "             'I-positive': 2,\n",
    "             'I-neutral': 4,\n",
    "             'I-negative': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_and_bio(rev_dict):\n",
    "    asp_labels = []\n",
    "    docs = []\n",
    "    keys = []\n",
    "    for k, review in rev_dict.items():\n",
    "        keys.append(k)\n",
    "        doc = Doc(review['text'])\n",
    "        doc.segment(segmenter)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        docs.append(doc)\n",
    "        asp_labels_local = []\n",
    "        for token in doc.tokens:\n",
    "            label = 'O'\n",
    "            for span in review['spans']:\n",
    "                if span[3] != 'both':\n",
    "                    if token.start == span[0]:\n",
    "                        label = f'B-{span[3]}'\n",
    "                    elif token.start > span[0] and token.stop <= span[1]:\n",
    "                        label = f'I-{span[3]}'\n",
    "            l = tag_table[label]\n",
    "            asp_labels_local.append(l)\n",
    "        asp_labels.append(asp_labels_local)\n",
    "    return docs, asp_labels, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs, train_asp_labels, train_keys = get_docs_and_bio(senti_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs, test_asp_labels, test_keys = get_docs_and_bio(senti_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tags = ['ADJ', 'ADP', 'ADV', 'AUX',\n",
    "            'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
    "            'NUM', 'PART', 'PRON', 'PROPN',\n",
    "            'PUNCT', 'SCONJ', 'SYM', 'VERB', \n",
    "            'X', '[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEP_tags = 'root, acl, acl:relcl, advcl, advmod, amod, appos, aux, aux:pass, case, cc, ccomp, compound, conj, cop, csubj, csubj:pass, dep, det, discourse, expl, fixed, flat, flat:foreign, flat:name, iobj, list, mark, nmod, nsubj, nsubj:pass, nummod, nummod:entity, nummod:gov, obj, obl, obl:agent, orphan, parataxis, punct, xcomp'.split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'blanchefort/rubert-base-cased-sentiment-rurewiews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(docs, asp_labels):\n",
    "    \n",
    "    examples = {}\n",
    "    tokens_lists = []\n",
    "    examples['tail_ids'] = []\n",
    "    examples['head_ids'] = []\n",
    "    examples['pos_tags'] = []\n",
    "    examples['dep_tags'] = []\n",
    "    examples['asp_tags'] = asp_labels\n",
    "    \n",
    "    for doc in docs:\n",
    "        tokens = []\n",
    "        tail_ids = []\n",
    "        head_ids = []\n",
    "        pos_tags = []\n",
    "        dep_tags = []\n",
    "        for token in doc.tokens:\n",
    "            tokens.append(token.text)\n",
    "            tail_ids.append(token.id)\n",
    "            head_ids.append(token.head_id)\n",
    "            pos_tags.append(POS_tags.index(token.pos))\n",
    "            dep_tags.append(DEP_tags.index(token.rel))\n",
    "        tokens_lists.append(tokens)\n",
    "        examples['tail_ids'].append(tail_ids)\n",
    "        examples['head_ids'].append(head_ids)\n",
    "        examples['pos_tags'].append(pos_tags)\n",
    "        examples['dep_tags'].append(dep_tags)\n",
    "        \n",
    "    tokenized_inputs = tokenizer(\n",
    "        tokens_lists, truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    \n",
    "    for label_type, tags in examples.items():\n",
    "        labels = []\n",
    "        for i, label in enumerate(tags):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            previous_word_idx = None\n",
    "            label_ids = []\n",
    "            for word_idx in word_ids:\n",
    "                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "                # ignored in the loss function.\n",
    "                if word_idx is None:\n",
    "                    label_ids.append(-100)\n",
    "                # We set the label for the first token of each word.\n",
    "                elif word_idx != previous_word_idx:\n",
    "                    label_ids.append(label[word_idx])\n",
    "                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "                # the label_all_tokens flag.\n",
    "                else:\n",
    "                    label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                previous_word_idx = word_idx\n",
    "\n",
    "            labels.append(label_ids)\n",
    "\n",
    "        tokenized_inputs[label_type] = labels\n",
    "        \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = tokenize_and_align_labels(train_docs, train_asp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = tokenize_and_align_labels(test_docs, test_asp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dep_matrices = []\n",
    "test_dep_matrices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "for heads, tails, deps in zip(train_tokenized['head_ids'], \n",
    "                              train_tokenized['tail_ids'], \n",
    "                              train_tokenized['dep_tags']):\n",
    "    dep_matrix = np.zeros((len(heads[1:-1]), len(heads[1:-1])))\n",
    "    for i, hid in enumerate(heads[1:-1]):\n",
    "        for j, tid in enumerate(tails[1:-1]):\n",
    "            if hid == tid: # j <- i\n",
    "                dep_matrix[j, i] = deps[1:-1][i]\n",
    "    train_dep_matrices.append(dep_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for heads, tails, deps in zip(test_tokenized['head_ids'], \n",
    "                              test_tokenized['tail_ids'], \n",
    "                              test_tokenized['dep_tags']):\n",
    "    dep_matrix = np.zeros((len(heads[1:-1]), len(heads[1:-1])))\n",
    "    for i, hid in enumerate(heads[1:-1]):\n",
    "        for j, tid in enumerate(tails[1:-1]):\n",
    "            if hid == tid: # j <- i\n",
    "                dep_matrix[j, i] = deps[1:-1][i]\n",
    "    test_dep_matrices.append(dep_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
